{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN模型预测版图寄生电容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faulthandler\n",
    "\n",
    "\n",
    "faulthandler.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "from config import *\n",
    "\n",
    "\n",
    "sys.argv = ['run.py']\n",
    "\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "        return True\n",
    "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "        return False\n",
    "    else:\n",
    "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dir_prj', type=str, default=dir_prj,\n",
    "                    help='project directory')\n",
    "parser.add_argument('--seed', type=int, default=seed,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--pattern_nums', type=int, nargs='+', default=pattern_nums,\n",
    "                    help='pattern nums')\n",
    "parser.add_argument('--nodes_range', type=int, nargs='+', default=nodes_range,\n",
    "                    help='the range of number of nodes')\n",
    "parser.add_argument('--num_process', type=int, default=num_process,\n",
    "                    help='multiprocessing number')\n",
    "parser.add_argument('--ndm', type=int, default=ndm,\n",
    "                    help='number of neighbor distance maximum')\n",
    "parser.add_argument('--k', type=int, default=K,\n",
    "                    help='number of neighbors')\n",
    "parser.add_argument('--model_name', type=str, default=model_name,\n",
    "                    help='model name [gcn, graph_sage, gat]')\n",
    "parser.add_argument('--lr', type=float, default=lr,\n",
    "                    help='adam learning rate')\n",
    "parser.add_argument('--batch_size', type=int, default=batch_size,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--epochs', type=int, default=epochs,\n",
    "                    help='number of epochs')\n",
    "parser.add_argument('--load_params', type=str2bool, default=LOAD_PARAMS,\n",
    "                    help='load parameters')\n",
    "parser.add_argument('--set_memory_growth', type=str2bool, default=SET_MEMORY_GROWTH,\n",
    "                    help='set memory growth')\n",
    "parser.add_argument('--set_memory_limit', type=int, default=SET_MEMORY_LIMIT,\n",
    "                    help='memory limit, -1 for no limit')\n",
    "parser.add_argument('--set_multi_gpu_num', type=int, default=SET_MULTI_GPU_NUM,\n",
    "                    help='set multi gpu numbers')\n",
    "parser.add_argument('-nt', '--no_train', action='store_true',\n",
    "                    help='no train')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "dir_prj = args.dir_prj\n",
    "seed = args.seed\n",
    "pattern_nums = args.pattern_nums\n",
    "nodes_range = args.nodes_range\n",
    "num_process = args.num_process\n",
    "ndm = args.ndm\n",
    "K = args.k\n",
    "model_name = args.model_name\n",
    "lr = args.lr\n",
    "batch_size = args.batch_size\n",
    "epochs = args.epochs\n",
    "LOAD_PARAMS = args.load_params\n",
    "SET_MEMORY_GROWTH = args.set_memory_growth\n",
    "SET_MEMORY_LIMIT = args.set_memory_limit\n",
    "SET_MULTI_GPU_NUM = args.set_multi_gpu_num\n",
    "SET_MULTI_GPU_NUM = min(SET_MULTI_GPU_NUM, 4)\n",
    "NO_TRAIN = args.no_train\n",
    "TRAIN = not NO_TRAIN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 路径定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# log save path\n",
    "dir_logs = os.path.join(os.getcwd(), '../logs')\n",
    "if not os.path.exists(dir_logs):\n",
    "    os.mkdir(dir_logs)\n",
    "\n",
    "# params save path\n",
    "dir_params = os.path.join(os.getcwd(), '../params')\n",
    "if not os.path.exists(dir_params):\n",
    "    os.mkdir(dir_params)\n",
    "\n",
    "# results save path\n",
    "dir_results = os.path.join(os.getcwd(), '../results')\n",
    "if not os.path.exists(dir_results):\n",
    "    os.mkdir(dir_results)\n",
    "\n",
    "# tensorboard save path\n",
    "dir_tensorboard = os.path.join(os.getcwd(), '../tensorboard')\n",
    "if not os.path.exists(dir_tensorboard):\n",
    "    os.mkdir(dir_tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 09:55:55 1019029065.py [line:17] INFO ------------------------args start----------------------------\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO dir_prj = D:/learn_more_from_life/computer/EDA/work/prj/rc_predict/\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO seed = 42\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO pattern_nums = [26]\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO nodes_range = [50, 1500]\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO num_process = 8\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO ndm = 25\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO k = 20\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO model_name = gcn\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO lr = 0.001\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO batch_size = 16\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO epochs = 20\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO load_params = False\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO set_memory_growth = True\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO set_memory_limit = -1\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO set_multi_gpu_num = 1\n",
      "2025-05-18 09:55:55 1019029065.py [line:19] INFO no_train = False\n",
      "2025-05-18 09:55:55 1019029065.py [line:20] INFO -------------------------args end-----------------------------\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "save_name = f'{model_name}_lr{lr}_batchsize{batch_size}_epochs{epochs}_k{K}_seed{seed}'\n",
    "console_handler = logging.StreamHandler()\n",
    "file_handler = logging.FileHandler(os.path.join(dir_logs, (f'{save_name}.log')), mode='w', encoding='utf-8')\n",
    "\n",
    "# 设置日志格式\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(filename)s [line:%(lineno)d] %(levelname)s %(message)s\",\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    handlers=[console_handler, file_handler],\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "# loging args\n",
    "logging.info('------------------------args start----------------------------')\n",
    "for k, v in vars(args).items():\n",
    "    logging.info(f'{k} = {v}')\n",
    "logging.info('-------------------------args end-----------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflow框架设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多卡设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "gpus = ''\n",
    "for i in range(SET_MULTI_GPU_NUM):\n",
    "    if i == 0:\n",
    "        gpus += str(i)\n",
    "    else:\n",
    "        gpus += ',' + str(i)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log level 设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 显存限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 09:56:05 3325873264.py [line:9] INFO [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# 方法一 set memory growth\n",
    "if SET_MEMORY_GROWTH:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        try:\n",
    "            logging.info(gpus[:SET_MULTI_GPU_NUM])\n",
    "            for gpu in gpus[:SET_MULTI_GPU_NUM]:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            logging.error(e)\n",
    "\n",
    "# 方法二 set memory limit\n",
    "if SET_MEMORY_LIMIT > 0:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        try:\n",
    "            logging.info(gpus[:SET_MULTI_GPU_NUM])\n",
    "            for gpu in gpus[:SET_MULTI_GPU_NUM]:\n",
    "                tf.config.experimental.set_virtual_device_configuration(\n",
    "                    gpu,\n",
    "                    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=SET_MEMORY_LIMIT)])\n",
    "            logging.info(f'set memory limit to {SET_MEMORY_LIMIT}MB')\n",
    "        except RuntimeError as e:\n",
    "            logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 库导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# 自定义模块导入\n",
    "from data.dataset import MyDataset\n",
    "from models.gcn import GCN\n",
    "from models.graph_sage import GraphSage\n",
    "from models.gat import GAT\n",
    "from models.gin import GIN\n",
    "\n",
    "# fix seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据导入与数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 09:56:08 1662972872.py [line:2] INFO ---------------------loading total data-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pattern26 data filtered, 0 data removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data from pattern26:   0%|          | 0/1500 [00:00<?, ?file/s]"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "logging.info('---------------------loading total data-----------------------')\n",
    "dataset_total = MyDataset(dir_prj=dir_prj,\n",
    "                          ndm=ndm,\n",
    "                          k=K,\n",
    "                          pattern_nums=pattern_nums,\n",
    "                          x_name='x_total.npy',\n",
    "                          y_name='y_total.npy',\n",
    "                          g_name='total',\n",
    "                          num_process=num_process,\n",
    "                          update=False)\n",
    "logging.info('---------------------load total data done---------------------')\n",
    "\n",
    "logging.info('---------------------loading couple data----------------------')\n",
    "dataset_couple = MyDataset(dir_prj=dir_prj,\n",
    "                           ndm=ndm,\n",
    "                           k=K, \n",
    "                           pattern_nums=pattern_nums,\n",
    "                           x_name='x_couple.npy',\n",
    "                           y_name='y_couple.npy',\n",
    "                           g_name='couple',\n",
    "                           num_process=num_process,\n",
    "                           update=False)\n",
    "logging.info('---------------------load couple data done--------------------')\n",
    "\n",
    "# clean data\n",
    "logging.info('------------------------before cleaning-----------------------')\n",
    "logging.info(f'dataset total: {dataset_total}')\n",
    "logging.info(f'dataset couple: {dataset_couple}')\n",
    "logging.info('------------------------after cleaning------------------------')\n",
    "dataset_total.filter(lambda g: nodes_range[0] <= g.n_nodes <= nodes_range[1])\n",
    "dataset_couple.filter(lambda g: nodes_range[0] <= g.n_nodes <= nodes_range[1])\n",
    "logging.info(f'dataset total: {dataset_total}')\n",
    "logging.info(f'dataset couple: {dataset_couple}')\n",
    "print(f'total x shape: {dataset_total[0].x.shape} sample: {dataset_total[0].x[:5]}')\n",
    "print(f'couple x shape: {dataset_couple[0].x.shape} sample: {dataset_couple[0].x[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data # 6:2:2\n",
    "np.random.shuffle(dataset_total)\n",
    "np.random.shuffle(dataset_couple)\n",
    "n = len(dataset_total)\n",
    "train_data_total, valid_data_total, test_data_total = dataset_total[0:int(n*0.6)], \\\n",
    "    dataset_total[int(n*0.6):int(n*0.8)], \\\n",
    "    dataset_total[int(n*0.8):]\n",
    "n = len(dataset_couple)\n",
    "train_data_couple, valid_data_couple, test_data_couple = dataset_couple[0:int(n * 0.6)], \\\n",
    "    dataset_couple[int(n * 0.6):int(n * 0.8)], \\\n",
    "    dataset_couple[int(n * 0.8):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mean_squared_error\n",
    "from spektral.data import BatchLoader\n",
    "from utils.model import huber_loss, mse_msre_loss, measure_ratio_bad, measure_ratio_good\n",
    "\n",
    "\n",
    "# loss and weighted metrics\n",
    "loss_func = mse_msre_loss\n",
    "weighted_metrics = [measure_ratio_good]\n",
    "\n",
    "# set multi gpu\n",
    "if SET_MULTI_GPU_NUM > 1:\n",
    "    devices = [f'/gpu:{i}' for i in range(SET_MULTI_GPU_NUM)]\n",
    "    mirror_strategy = tf.distribute.MirroredStrategy(devices=devices)\n",
    "    with mirror_strategy.scope():\n",
    "        logging.info(\n",
    "            '---------------------set multi gpu done---------------------')\n",
    "        logging.info(f'num devices: {mirror_strategy.num_replicas_in_sync}')\n",
    "        logging.info(f'devices: {devices}')\n",
    "        logging.info(\n",
    "            '------------------------------------------------------------')\n",
    "        # GCN model\n",
    "        if model_name == 'gcn':\n",
    "            model_total = GCN(training=True)\n",
    "            model_couple = GCN(training=True)\n",
    "            model_best_total = GCN(training=True)\n",
    "            model_best_couple = GCN(training=True)\n",
    "            logging.info('buld model gcn done')\n",
    "        # GraphSAGE model\n",
    "        elif model_name == 'graph_sage':\n",
    "            model_total = GraphSage(training=True)\n",
    "            model_couple = GraphSage(training=True)\n",
    "            model_best_total = GraphSage(training=True)\n",
    "            model_best_couple = GraphSage(training=True)\n",
    "            logging.info('buld model graph_sage done')\n",
    "        # GAT model\n",
    "        elif model_name == 'gat':\n",
    "            model_total = GAT(training=True)\n",
    "            model_couple = GAT(training=True)\n",
    "            model_best_total = GAT(training=True)\n",
    "            model_best_couple = GAT(training=True)\n",
    "            logging.info('buld model gat done')\n",
    "        # GIN model\n",
    "        elif model_name == 'gin':\n",
    "            model_total = GIN(training=True)\n",
    "            model_couple = GIN(training=True)\n",
    "            model_best_total = GIN(training=True)\n",
    "            model_best_couple = GIN(training=True)\n",
    "            logging.info('buld model gin done')\n",
    "        else:\n",
    "            logging.error('model_name error')\n",
    "            raise ValueError('model_name error')\n",
    "        model_total.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                            loss=loss_func,\n",
    "                            weighted_metrics=weighted_metrics)\n",
    "        model_couple.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                             loss=loss_func,\n",
    "                             weighted_metrics=weighted_metrics)\n",
    "        # best model\n",
    "        model_best_total.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                                 loss=loss_func,\n",
    "                                 weighted_metrics=weighted_metrics)\n",
    "        model_best_couple.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                                  loss=loss_func,\n",
    "                                  weighted_metrics=weighted_metrics)\n",
    "else:\n",
    "    # GCN model\n",
    "    if model_name == 'gcn':\n",
    "        model_total = GCN(training=True)\n",
    "        model_couple = GCN(training=True)\n",
    "        model_best_total = GCN(training=True)\n",
    "        model_best_couple = GCN(training=True)\n",
    "        logging.info('buld model gcn done')\n",
    "    # GraphSAGE model\n",
    "    elif model_name == 'graph_sage':\n",
    "        model_total = GraphSage(training=True)\n",
    "        model_couple = GraphSage(training=True)\n",
    "        model_best_total = GraphSage(training=True)\n",
    "        model_best_couple = GraphSage(training=True)\n",
    "        logging.info('buld model graph_sage done')\n",
    "    # GAT model\n",
    "    elif model_name == 'gat':\n",
    "        model_total = GAT(training=True)\n",
    "        model_couple = GAT(training=True)\n",
    "        model_best_total = GAT(training=True)\n",
    "        model_best_couple = GAT(training=True)\n",
    "        logging.info('buld model gat done')\n",
    "    # GIN model\n",
    "    elif model_name == 'gin':\n",
    "        model_total = GIN(training=True)\n",
    "        model_couple = GIN(training=True)\n",
    "        model_best_total = GIN(training=True)\n",
    "        model_best_couple = GIN(training=True)\n",
    "        logging.info('buld model gin done')\n",
    "    else:\n",
    "        logging.error('model_name error')\n",
    "        raise ValueError('model_name error')\n",
    "\n",
    "    model_total.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                        loss=loss_func,\n",
    "                        weighted_metrics=weighted_metrics)\n",
    "    model_couple.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                         loss=loss_func,\n",
    "                         weighted_metrics=weighted_metrics)\n",
    "    # best model\n",
    "    model_best_total.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                             loss=loss_func,\n",
    "                             weighted_metrics=weighted_metrics)\n",
    "    model_best_couple.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                              loss=loss_func,\n",
    "                              weighted_metrics=weighted_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_total_load_path = os.path.join(dir_params, f'total_{save_name}.h5')\n",
    "model_couple_load_path = os.path.join(dir_params, f'couple_{save_name}.h5')\n",
    "\n",
    "# load model\n",
    "if os.path.exists(model_total_load_path) and LOAD_PARAMS:\n",
    "    loader = BatchLoader(\n",
    "        train_data_total[:batch_size], batch_size=batch_size, shuffle=True)\n",
    "    model_total.fit(loader.load(),\n",
    "                    steps_per_epoch=1,\n",
    "                    epochs=1)\n",
    "    try:\n",
    "        model_total.load_weights(model_total_load_path)\n",
    "        logging.info('load model total done')\n",
    "    except:\n",
    "        logging.error('load model total failed')\n",
    "if os.path.exists(model_couple_load_path) and LOAD_PARAMS:\n",
    "    loader = BatchLoader(\n",
    "        train_data_couple[:batch_size], batch_size=batch_size, shuffle=True)\n",
    "    model_couple.fit(loader.load(),\n",
    "                     steps_per_epoch=1,\n",
    "                     epochs=1)\n",
    "    try:\n",
    "        model_couple.load_weights(model_couple_load_path)\n",
    "        logging.info('load model couple done')\n",
    "    except:\n",
    "        logging.error('load model couple failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集总电容模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 手动记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_TENSORBOARD and TRAIN:\n",
    "    from spektral.data import BatchLoader\n",
    "    from utils.model import sync\n",
    "\n",
    "    loader_train = BatchLoader(\n",
    "        train_data_total, batch_size=batch_size, shuffle=True)\n",
    "    loader_valid = BatchLoader(\n",
    "        valid_data_total, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_score = 1e10\n",
    "\n",
    "    # train model\n",
    "    loss_all = []\n",
    "    val_loss_all = []\n",
    "    start = time.time()\n",
    "    logging.info('----------------model total train start------------------')\n",
    "    for epoch in range(epochs):\n",
    "        history = model_total.fit(loader_train.load(),\n",
    "                                  validation_data=loader_valid.load(),\n",
    "                                  steps_per_epoch=loader_train.steps_per_epoch,\n",
    "                                  validation_steps=loader_valid.steps_per_epoch,\n",
    "                                  epochs=1,\n",
    "                                  shuffle=False)\n",
    "        loss_all.append(history.history['loss'][0])\n",
    "        val_loss_all.append(history.history['val_loss'][0])\n",
    "\n",
    "        # log\n",
    "        logging.info(\n",
    "            f'total model:{model_name} epoch:{epoch} loss:{loss_all[-1]} val_loss:{val_loss_all[-1]}')\n",
    "\n",
    "        # save model every model_save_freq epochs\n",
    "        if (epoch + 1) % model_save_freq == 0:\n",
    "            model_total_save_path = os.path.join(\n",
    "                dir_params, f'total_{save_name}.h5')\n",
    "            model_total.save_weights(model_total_save_path)\n",
    "\n",
    "        # save best model on validation set\n",
    "        val_loss, val_score = model_total.evaluate(\n",
    "            loader_valid.load(), steps=loader_valid.steps_per_epoch)\n",
    "        if val_score < best_val_score:\n",
    "            best_epoch = epoch\n",
    "            best_val_score = val_score\n",
    "            sync(model_total, model_best_total)\n",
    "    end = time.time()\n",
    "    logging.info(\n",
    "        f'model total train done epoch: {epochs}, time: {end - start}s')\n",
    "    logging.info('-----------------model total train end-------------------')\n",
    "\n",
    "    # save best model\n",
    "    model_total_save_path = os.path.join(\n",
    "        dir_params, f'total_best_{save_name}.h5')\n",
    "    model_best_total.save_weights(model_total_save_path)\n",
    "    model_total.summary()\n",
    "\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.plot(loss_all, label='train loss')\n",
    "    plt.plot(val_loss_all, label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    # save picture\n",
    "    plt_save_path = os.path.join(\n",
    "        dir_results, f\"total_{save_name}.jpg\")\n",
    "    plt.savefig(plt_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorboard使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TENSORBOARD and TRAIN:\n",
    "    from spektral.data import BatchLoader\n",
    "\n",
    "    loader_train = BatchLoader(\n",
    "        train_data_total, batch_size=batch_size, shuffle=True)\n",
    "    loader_valid = BatchLoader(\n",
    "        valid_data_total, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    tb_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(dir_tensorboard, f'total_{save_name}'))\n",
    "    log_callback = tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: logging.info(\n",
    "            f'total model:{model_name} epoch:{epoch} loss:{logs[\"loss\"]} val_loss:{logs[\"val_loss\"]}')\n",
    "    )\n",
    "    save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\n",
    "            dir_params, f'total_best_{save_name}.h5'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='auto',\n",
    "        period=1\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    start = time.time()\n",
    "    logging.info('----------------model total train start------------------')\n",
    "    history = model_total.fit(loader_train.load(),\n",
    "                              validation_data=loader_valid.load(),\n",
    "                              steps_per_epoch=loader_train.steps_per_epoch,\n",
    "                              validation_steps=loader_valid.steps_per_epoch,\n",
    "                              epochs=epochs,\n",
    "                              shuffle=False,\n",
    "                              callbacks=[tb_callback, log_callback, save_callback])\n",
    "\n",
    "    end = time.time()\n",
    "    logging.info(\n",
    "        f'model total train done epoch: {epochs}, time: {end - start}s')\n",
    "    logging.info('-----------------model total train end-------------------')\n",
    "\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train loss')\n",
    "    plt.plot(history.history['val_loss'], label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    # save picture\n",
    "    plt_save_path = os.path.join(\n",
    "        dir_results, f\"total_{save_name}.jpg\")\n",
    "    plt.savefig(plt_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import gnn_analysis, gnn_plot, test_runtime\n",
    "\n",
    "\n",
    "# best model load\n",
    "# build model\n",
    "loader = BatchLoader(\n",
    "    train_data_total[:batch_size], batch_size=batch_size, shuffle=True)\n",
    "model_total = GCN(training=False) if model_name == 'gcn' else GraphSage(\n",
    "    training=False) if model_name == 'graph_sage' else GAT(\n",
    "    training=False) if model_name == 'gat' else GIN(\n",
    "    training=False) if model_name == 'gin' else None\n",
    "model_total.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                    loss=loss_func,\n",
    "                    weighted_metrics=weighted_metrics)\n",
    "model_total.fit(loader.load(),\n",
    "                steps_per_epoch=1,\n",
    "                epochs=1)\n",
    "model_total_load_path = os.path.join(\n",
    "    dir_params, f'total_best_{save_name}.h5')\n",
    "if os.path.exists(model_total_load_path):\n",
    "    model_total.load_weights(model_total_load_path)\n",
    "else:\n",
    "    raise ValueError('model total load path does not exist')\n",
    "model_total.summary()\n",
    "\n",
    "yt_train = np.array([data.y for data in train_data_total]).reshape(-1, 1)\n",
    "yt_valid = np.array([data.y for data in valid_data_total]).reshape(-1, 1)\n",
    "yt_test = np.array([data.y for data in test_data_total]).reshape(-1, 1)\n",
    "\n",
    "# get table\n",
    "dict_total = gnn_analysis(model_total, batch_size,\n",
    "                          train_data_total, yt_train,\n",
    "                          valid_data_total, yt_valid,\n",
    "                          test_data_total, yt_test, name=model_name)\n",
    "data_total = pd.Series(dict_total).to_frame(name='total').T\n",
    "\n",
    "# scatter plot\n",
    "gnn_plot(model_total, batch_size,\n",
    "         train_data_total, yt_train,\n",
    "         valid_data_total, yt_valid,\n",
    "         test_data_total, yt_test, dir=dir_results,\n",
    "         name=f'total_{save_name}')\n",
    "\n",
    "# runtime test\n",
    "total_avg_time = test_runtime(model_total, batch_size,\n",
    "                              test_data_total, yt_test)\n",
    "\n",
    "# save results\n",
    "data_total.to_csv(os.path.join(\n",
    "    dir_results, f\"total_{save_name}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 耦合电容模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 手动记录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not USE_TENSORBOARD) and TRAIN:\n",
    "    from spektral.data import BatchLoader\n",
    "    from utils.model import sync\n",
    "\n",
    "    loader_train = BatchLoader(\n",
    "        train_data_couple, batch_size=batch_size, shuffle=True)\n",
    "    loader_valid = BatchLoader(\n",
    "        valid_data_couple, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_epoch = 0\n",
    "    best_val_score = 1e10\n",
    "\n",
    "    # train model\n",
    "    loss_all = []\n",
    "    val_loss_all = []\n",
    "    start = time.time()\n",
    "    logging.info('----------------model couple train start------------------')\n",
    "    for epoch in range(epochs):\n",
    "        history = model_couple.fit(loader_train.load(),\n",
    "                                   validation_data=loader_valid.load(),\n",
    "                                   steps_per_epoch=loader_train.steps_per_epoch,\n",
    "                                   validation_steps=loader_valid.steps_per_epoch,\n",
    "                                   epochs=1,\n",
    "                                   shuffle=False)\n",
    "        loss_all.append(history.history['loss'][0])\n",
    "        val_loss_all.append(history.history['val_loss'][0])\n",
    "\n",
    "        # log\n",
    "        logging.info(\n",
    "            f'couple model:{model_name} epoch:{epoch} loss:{loss_all[-1]} val_loss:{val_loss_all[-1]}')\n",
    "\n",
    "        # save model every model_save_freq epochs\n",
    "        if (epoch + 1) % model_save_freq == 0:\n",
    "            model_couple_save_path = os.path.join(\n",
    "                dir_params, f'couple_{save_name}.h5')\n",
    "            model_couple.save_weights(model_couple_save_path)\n",
    "\n",
    "        # save best model on validation set\n",
    "        val_loss, val_score = model_couple.evaluate(\n",
    "            loader_valid.load(), steps=loader_valid.steps_per_epoch)\n",
    "        if val_score < best_val_score:\n",
    "            best_epoch = epoch\n",
    "            best_val_score = val_score\n",
    "            sync(model_couple, model_best_couple)\n",
    "    end = time.time()\n",
    "    logging.info(\n",
    "        f'model couple train done epoch: {epochs}, time: {end - start}s')\n",
    "    logging.info('-----------------model couple train end-------------------')\n",
    "\n",
    "    # save best model\n",
    "    model_couple_save_path = os.path.join(\n",
    "        dir_params, f'couple_best_{save_name}.h5')\n",
    "    model_best_couple.save_weights(model_couple_save_path)\n",
    "    model_couple.summary()\n",
    "\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.plot(loss_all, label='train loss')\n",
    "    plt.plot(val_loss_all, label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    # save picture\n",
    "    plt_save_path = os.path.join(\n",
    "        dir_results, f\"couple_{save_name}.jpg\")\n",
    "    plt.savefig(plt_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tensorboard 使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_TENSORBOARD and TRAIN:\n",
    "    from spektral.data import BatchLoader\n",
    "\n",
    "    loader_train = BatchLoader(\n",
    "        train_data_couple, batch_size=batch_size, shuffle=True)\n",
    "    loader_valid = BatchLoader(\n",
    "        valid_data_couple, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    tf_callback = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(dir_tensorboard, f'couple_{save_name}'))\n",
    "    log_callback = tf.keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: logging.info(\n",
    "            f'couple model:{model_name} epoch:{epoch} loss:{logs[\"loss\"]} val_loss:{logs[\"val_loss\"]}')\n",
    "    )\n",
    "    save_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\n",
    "            dir_params, f'couple_best_{save_name}.h5'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode='auto',\n",
    "        period=1\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    logging.info('----------------model couple train start------------------')\n",
    "    history = model_couple.fit(loader_train.load(),\n",
    "                               validation_data=loader_valid.load(),\n",
    "                               steps_per_epoch=loader_train.steps_per_epoch,\n",
    "                               validation_steps=loader_valid.steps_per_epoch,\n",
    "                               epochs=epochs,\n",
    "                               shuffle=False,\n",
    "                               callbacks=[tf_callback, log_callback, save_callback])\n",
    "\n",
    "    end = time.time()\n",
    "    logging.info(\n",
    "        f'model couple train done epoch: {epochs}, time: {end - start}s')\n",
    "    logging.info('-----------------model couple train end-------------------')\n",
    "\n",
    "    # plot\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train loss')\n",
    "    plt.plot(history.history['val_loss'], label='valid loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "\n",
    "    # save picture\n",
    "    plt_save_path = os.path.join(\n",
    "        dir_results, f\"couple_{save_name}.jpg\")\n",
    "    plt.savefig(plt_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import gnn_analysis\n",
    "\n",
    "\n",
    "# load best model\n",
    "# build model\n",
    "loader = BatchLoader(\n",
    "    train_data_couple[:batch_size], batch_size=batch_size, shuffle=True)\n",
    "model_couple = GCN(training=False) if model_name == 'gcn' else GraphSage(\n",
    "    training=False) if model_name == 'graph_sage' else GAT(\n",
    "    training=False) if model_name == 'gat' else GIN(\n",
    "    training=False) if model_name == 'gin' else None\n",
    "model_couple.compile(optimizer=Adam(learning_rate=lr, epsilon=epsilon),\n",
    "                     loss=loss_func,\n",
    "                     weighted_metrics=weighted_metrics)\n",
    "model_couple.fit(loader.load(),\n",
    "                 steps_per_epoch=1,\n",
    "                 epochs=1)\n",
    "model_couple_save_path = os.path.join(\n",
    "    dir_params, f'couple_best_{save_name}.h5')\n",
    "if os.path.exists(model_couple_save_path):\n",
    "    model_couple.load_weights(model_couple_save_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"model_couple_save_path: {model_couple_save_path} not found!\")\n",
    "\n",
    "yc_train = np.array([data.y for data in train_data_couple]).reshape(-1, 1)\n",
    "yc_valid = np.array([data.y for data in valid_data_couple]).reshape(-1, 1)\n",
    "yc_test = np.array([data.y for data in test_data_couple]).reshape(-1, 1)\n",
    "\n",
    "# get table\n",
    "dict_couple = gnn_analysis(model_couple, batch_size,\n",
    "                           train_data_couple, yc_train,\n",
    "                           valid_data_couple, yc_valid,\n",
    "                           test_data_couple, yc_test, name=model_name)\n",
    "data_couple = pd.Series(dict_couple).to_frame(name='couple').T\n",
    "\n",
    "# scatter plot\n",
    "gnn_plot(model_couple, batch_size,\n",
    "         train_data_couple, yc_train,\n",
    "         valid_data_couple, yc_valid,\n",
    "         test_data_couple, yc_test, dir=dir_results,\n",
    "         name=f'couple_{save_name}')\n",
    "\n",
    "# runtime test\n",
    "couple_avg_time = test_runtime(model_couple, batch_size,\n",
    "                                test_data_couple, yc_test)\n",
    "# all time\n",
    "avg_time = total_avg_time + couple_avg_time\n",
    "logging.info(f'all avg time: {avg_time}')\n",
    "\n",
    "# save results\n",
    "data_couple.to_csv(os.path.join(\n",
    "    dir_results, f\"couple_{save_name}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

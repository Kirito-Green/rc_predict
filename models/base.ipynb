{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be54000d",
   "metadata": {},
   "source": [
    "# 版图电容预测基本模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58824f0",
   "metadata": {},
   "source": [
    "## 基本库导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcbf9af3-b562-46b5-9c58-e09b93586804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "from config import *\n",
    "from data.layout import convert_data_parallel\n",
    "from utils.analysis import ratio_good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2611f84a",
   "metadata": {},
   "source": [
    "## 数据导入与数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823851f3-62b4-44c8-bcdf-cbb46a45f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process2(x, y, thresh): # [400 2500]\n",
    "\tthresh = np.array(thresh, dtype=np.int32)\n",
    "\traw_num = np.array([len(i) for i in x]).reshape(-1, 1)\n",
    "\tvalid_num = np.array([thresh[1] if len(i) > thresh[1] else len(i) for i in x], dtype=np.int32).reshape(-1, 1)\n",
    "\tsum_reserve = np.sum(valid_num >= thresh[0], dtype=np.int32)\n",
    "\tnew_x = np.zeros(shape=(sum_reserve, thresh[1], 5), dtype=np.float32)\n",
    "\tnew_y = np.zeros(shape=(sum_reserve, 1), dtype=np.float32)\n",
    "\tindex = 0\n",
    "\tfor i, num in enumerate(valid_num):\n",
    "\t\tnum = num[0]\n",
    "\t\tif num >= thresh[0]:\n",
    "\t\t\tnew_x[index][:num] = x[i][:num]\n",
    "\t\t\tnew_y[index][0] = y[i][0]\n",
    "\t\t\tindex += 1\n",
    "\tvalid_num = valid_num[valid_num>=thresh[0]].reshape(-1, 1)\n",
    "\tnew_y = np.concatenate([valid_num, new_y], axis=1)\n",
    "\n",
    "\treturn new_x, new_y, raw_num\n",
    "\n",
    "x_total = []\n",
    "x_couple = []\n",
    "y_total = []\n",
    "y_couple = []\n",
    "raw_nums = []\n",
    "if len(pattern_nums) == 0:\n",
    "\tprint(\"pattern_nums is empty, please check the config.py\")\n",
    "for pattern_num in pattern_nums:\n",
    "\tdir_load = os.path.join(dir_prj, \"data/convert_data/pattern{}\".format(pattern_num))\n",
    "\tif not os.path.exists(dir_load):\n",
    "\t\t# convert_data(pattern_num)\n",
    "\t\tconvert_data_parallel(dir_prj, pattern_num, num_process=8)\n",
    "\n",
    "# data process 2\n",
    "x_total_ = np.load(os.path.join(dir_load, \"x_total.npy\"), allow_pickle=True)\n",
    "y_total_ = np.load(os.path.join(dir_load, \"y_total.npy\")).reshape(-1, 1)\n",
    "x_total_, y_total_, total_raw_nums = data_process2(x_total_, y_total_, thresh)\n",
    "x_couple_ = np.load(os.path.join(dir_load, \"x_couple.npy\"), allow_pickle=True)\n",
    "y_couple_ = np.load(os.path.join(dir_load, \"y_couple.npy\")).reshape(-1, 1)\n",
    "x_couple_, y_couple_, _ = data_process2(x_couple_, y_couple_, thresh)\n",
    "\n",
    "# concatenate data\n",
    "if len(x_total) == 0:\n",
    "\tx_total = x_total_.copy()\n",
    "\tx_couple = x_couple_.copy()\n",
    "\ty_total = y_total_.copy()\n",
    "\ty_couple = y_couple_.copy()\n",
    "\traw_nums = total_raw_nums.copy()\n",
    "else:\n",
    "\tx_total = np.concatenate([x_total, x_total_], axis=0)\n",
    "\tx_couple = np.concatenate([x_couple, x_couple_], axis=0)\n",
    "\ty_total = np.concatenate([y_total, y_total_], axis=0)\n",
    "\ty_couple = np.concatenate([y_couple, y_couple_], axis=0)\n",
    "\traw_nums = np.concatenate([raw_nums, total_raw_nums],  axis=0)\n",
    "print(\"load data from {}\".format(dir_load))\n",
    "\n",
    "print('raw nums shape:', raw_nums.shape)\n",
    "print('raw nums mean:', np.mean(raw_nums))\n",
    "print('raw nums max:', np.max(raw_nums))\n",
    "print('raw nums min:', np.min(raw_nums))\n",
    "\n",
    "valid_num = y_total[:, 0]\n",
    "print('valid nums shape:', valid_num.shape)\n",
    "print('valid nums mean:', np.mean(valid_num))\n",
    "print('valid nums max:', np.max(valid_num))\n",
    "print('valid nums min:', np.min(valid_num))\n",
    "\n",
    "print(x_total.shape)\n",
    "print('x total first 10 samples:')\n",
    "print(x_total[0][:10])\n",
    "print('x couple first 10 samples:')\n",
    "print(x_couple[0][:10])\n",
    "\n",
    "print(y_total.shape)\n",
    "print('y total first 10 samples:')\n",
    "print(y_total[:10])\n",
    "print('y couple first 10 samples:')\n",
    "print(y_couple[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92879dae",
   "metadata": {},
   "source": [
    "## 数据分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cc341-33f3-46c1-a418-1b2e1dd89ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data split 6:2:2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_total_train, x_total_valid_test, y_total_train, y_total_valid_test = train_test_split(x_total, y_total, test_size=0.4, random_state=seed, shuffle=True)\n",
    "x_total_valid, x_total_test, y_total_valid, y_total_test = train_test_split(x_total_valid_test, y_total_valid_test, test_size=0.5, random_state=seed, shuffle=True)\n",
    "print('total x len:', len(x_total_train), len(x_total_valid), len(x_total_test))\n",
    "print('x total train first 10\\n', x_total_train[0][:10])\n",
    "print('x total train last 10\\n', x_total_train[0][-10:])\n",
    "print('y total train first 10\\n', y_total_train[:10])\n",
    "\n",
    "x_couple_train, x_couple_valid_test, y_couple_train, y_couple_valid_test = train_test_split(x_couple, y_couple, test_size=0.4, random_state=seed, shuffle=True)\n",
    "x_couple_valid, x_couple_test, y_couple_valid, y_couple_test = train_test_split(x_couple_valid_test, y_couple_valid_test, test_size=0.5, random_state=seed, shuffle=True)\n",
    "print('couple x len', len(x_couple_train), len(x_couple_valid), len(x_couple_test))\n",
    "print('x couple train first 10\\n', x_couple_train[0][:10])\n",
    "print('x couple train last 10\\n', x_couple_train[0][-10:])\n",
    "print('y couple train first 10\\n', y_couple_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82806e19",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f08ea-6867-4e26-8bc0-b751a95df67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def cal_mean_std(x, y):\n",
    "\tnum_s = 0\n",
    "\tnum_sum = round(np.sum(y[:, 0]))\n",
    "\tx_valid = np.zeros((num_sum, 5), dtype=np.float32)\n",
    "\tfor i in range(len(x)):\n",
    "\t\tnum = round(y[i][0])\n",
    "\t\tx_valid[num_s:num_s+num, :] = x[i][:num]\n",
    "\t\tnum_s += num\n",
    "\tmean = np.mean(x_valid, axis=0)\n",
    "\tstd = np.std(x_valid, axis=0)\n",
    "\tstd[std == 0] = 1\n",
    "\n",
    "\treturn mean, std\n",
    "\n",
    "def get_mask(x, y):\n",
    "\tmask = np.zeros(shape=x.shape, dtype=np.int32)\n",
    "\tfor i in range(len(x)):\n",
    "\t\tnum = round(y[i][0])\n",
    "\t\tmask[i][:num] = 1\n",
    "\n",
    "\treturn mask\n",
    "\n",
    "def data_process(x, y, mean, std):\n",
    "\tmask = get_mask(x, y)\n",
    "\n",
    "\treturn np.multiply((x - mean) / std, mask)\n",
    "\n",
    "# x\n",
    "# total\n",
    "mean_total, std_total = cal_mean_std(x_total_train, y_total_train)\n",
    "print('mean total:', mean_total)\n",
    "print('std total:', std_total)\n",
    "x_total_train_std = data_process(x_total_train, y_total_train, mean_total, std_total)\n",
    "x_total_train_std_flat = x_total_train_std.reshape(len(x_total_train_std), -1)\n",
    "x_total_valid_std = data_process(x_total_valid, y_total_valid, mean_total, std_total)\n",
    "x_total_valid_std_flat = x_total_valid_std.reshape(len(x_total_valid_std), -1)\n",
    "x_total_test_std = data_process(x_total_test, y_total_test, mean_total, std_total)\n",
    "x_total_test_std_flat = x_total_test_std.reshape(len(x_total_test_std), -1)\n",
    "print('x total train std flat first 20\\n', x_total_train_std_flat[0][:20])\n",
    "print('x total train std flat last 20\\n', x_total_train_std_flat[0][-20:])\n",
    "\n",
    "# couple\n",
    "mean_couple, std_couple = cal_mean_std(x_couple_train, y_couple_train)\n",
    "print('mean couple:', mean_couple)\n",
    "print('std couple:', std_couple)\n",
    "x_couple_train_std = data_process(x_couple_train, y_couple_train, mean_couple, std_couple)\n",
    "x_couple_train_std_flat = x_couple_train_std.reshape(len(x_couple_train_std), -1)\n",
    "x_couple_valid_std = data_process(x_couple_valid, y_couple_valid, mean_couple, std_couple)\n",
    "x_couple_valid_std_flat = x_couple_valid_std.reshape(len(x_couple_valid_std), -1)\n",
    "x_couple_test_std = data_process(x_couple_test, y_couple_test, mean_couple, std_couple)\n",
    "x_couple_test_std_flat = x_couple_test_std.reshape(len(x_couple_test_std), -1)\n",
    "print('x couple train std flat first 20\\n', x_couple_train_std_flat[0][:20])\n",
    "print('x couple train std flat last 20\\n', x_couple_train_std_flat[0][-20:])\n",
    "\n",
    "# y\n",
    "# total\n",
    "scaler_yt = StandardScaler()\n",
    "y_total_train_std = y_total_train[:, 1].copy().reshape(-1, 1)\n",
    "# scaler_yt.fit(y_total_train_std)\n",
    "# y_total_train_std = scaler_yt.transform(y_total_train_std)\n",
    "# print('y total train std first 10\\n', y_total_train_std[:10].reshape(-1))\n",
    "\n",
    "# # couple\n",
    "scaler_yc = StandardScaler()\n",
    "y_couple_train_std = y_couple_train[:, 1].copy().reshape(-1, 1)\n",
    "# scaler_yc.fit(y_couple_train_std)\n",
    "# y_couple_train_std = scaler_yc.transform(y_couple_train_std)\n",
    "# print('y couple train std first 10\\n', y_couple_train_std[:10].reshape(-1))\n",
    "\n",
    "if DISABLE_STD:\n",
    "\tx_total_train_std_flat = x_total_train.reshape(len(x_total_train), -1)\n",
    "\tx_total_valid_std_flat = x_total_valid.reshape(len(x_total_valid), -1)\n",
    "\tx_total_test_std_flat = x_total_test.reshape(len(x_total_test), -1)\n",
    "\tx_couple_train_std_flat = x_couple_train.reshape(len(x_couple_train), -1)\n",
    "\tx_couple_valid_std_flat = x_couple_valid.reshape(len(x_couple_valid), -1)\n",
    "\tx_couple_test_std_flat = x_couple_test.reshape(len(x_couple_test), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0c64aa",
   "metadata": {},
   "source": [
    "## 降维处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a523d",
   "metadata": {},
   "source": [
    "#### 传统降维方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becaa495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始维度 10000维\n",
    "from sklearn import decomposition\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2\n",
    "\n",
    "\n",
    "if USE_DDR_PCA:\n",
    "\tpca_total = decomposition.PCA(n_components=n_components, copy=True, whiten=True)\n",
    "\tpca_total.fit(x_total_train_std_flat)\n",
    "\tpca_couple = decomposition.PCA(n_components=n_components, copy=True, whiten=True)\n",
    "\tpca_couple.fit(x_couple_train_std_flat)\n",
    "\tddr_total = pca_total\n",
    "\tddr_couple = pca_couple\n",
    "elif USE_DDR_KPCA:\n",
    "\tkpca_total = decomposition.KernelPCA(n_components=n_components, kernel='rbf')\n",
    "\tkpca_total.fit(x_total_train_std_flat)\n",
    "\tkpca_couple = decomposition.KernelPCA(n_components=n_components, kernel='rbf')\n",
    "\tkpca_couple.fit(x_couple_train_std_flat)\n",
    "\tddr_total = kpca_total\n",
    "\tddr_couple = kpca_couple\n",
    "elif USE_DDR_VAR:\n",
    "\tvar_total = VarianceThreshold(threshold=0.5)\n",
    "\tvar_total.fit(x_total_train_std_flat)\n",
    "\tvar_couple = VarianceThreshold(threshold=0.5)\n",
    "\tvar_couple.fit(x_couple_train_std_flat)\n",
    "\tddr_total = var_total\n",
    "\tddr_couple = var_couple\n",
    "else:\n",
    "\tddr_total = None\n",
    "\tddr_couple = None\n",
    "\n",
    "# transform\n",
    "if ddr_total is not None and ddr_couple is not None:\n",
    "\tx_total_train_std_flat_ddr = ddr_total.transform(x_total_train_std_flat)\n",
    "\tx_total_valid_std_flat_ddr = ddr_total.transform(x_total_valid_std_flat)\n",
    "\tx_total_test_std_flat_ddr = ddr_total.transform(x_total_test_std_flat)\n",
    "\tx_couple_train_std_flat_ddr = ddr_couple.transform(x_couple_train_std_flat)\n",
    "\tx_couple_valid_std_flat_ddr = ddr_couple.transform(x_couple_valid_std_flat)\n",
    "\tx_couple_test_std_flat_ddr = ddr_couple.transform(x_couple_test_std_flat)\n",
    "\n",
    "# plot\n",
    "# ratio = ddr_total.explained_variance_ratio_\n",
    "# cum_ratio = np.cumsum(ratio)\n",
    "# print(f'total cum ratio {cum_ratio}')\n",
    "# plt.plot(range(n_components), cum_ratio)\n",
    "# plt.show()\n",
    "# print(f'total sum ratio {np.sum(ratio)}')\n",
    "\n",
    "# ratio = ddr_couple.explained_variance_ratio_\n",
    "# cum_ratio = np.cumsum(ratio)\n",
    "# print(f'couple cum ratio {cum_ratio}')\n",
    "# plt.plot(range(n_components), cum_ratio)\n",
    "# plt.show()\n",
    "# print(f'couple sum ratio {np.sum(ratio)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6a93d9",
   "metadata": {},
   "source": [
    "#### 新型降维方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e2bab0",
   "metadata": {},
   "source": [
    "##### 自编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27405ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# encoding dim 100\n",
    "encoding_dim = n_components\n",
    "\n",
    "# input dim 10000\n",
    "input_dim = x_total_train_std_flat.shape[1]\n",
    "input_layer = keras.layers.Input(shape=(input_dim,))\n",
    "# encoding layer\n",
    "encoded = keras.layers.Dense(encoding_dim, activation='relu')(input_layer)\n",
    "# decoding layer\n",
    "decoded = keras.layers.Dense(input_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "# autoencoder\n",
    "autoencoder = keras.models.Model(input_layer, decoded)\n",
    "# encoder\n",
    "encoder = keras.models.Model(input_layer, encoded)\n",
    "# decoder\n",
    "encoded_input = keras.layers.Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = keras.models.Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# compile\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "if USE_DDR_AE:\n",
    "\t# fit\n",
    "\tautoencoder.fit(x_total_train_std_flat, x_total_train_std_flat,\n",
    "\t\t\t\t\t\t\t\t\tepochs=100,\n",
    "\t\t\t\t\t\t\t\t\tbatch_size=32,\n",
    "\t\t\t\t\t\t\t\t\tshuffle=True,\n",
    "\t\t\t\t\t\t\t\t\tvalidation_data=(x_total_valid_std_flat, x_total_valid_std_flat))\n",
    "\n",
    "\t# history\n",
    "\thistory = autoencoder.history.history\n",
    "\tplt.plot(history['loss'], label='train')\n",
    "\tplt.plot(history['val_loss'], label='valid')\n",
    "\tplt.legend()\n",
    "\tplt.xlabel('epoch')\n",
    "\tplt.ylabel('loss')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode data\n",
    "x_total_train_std_flat_ddr = encoder.predict(x_total_train_std_flat)\n",
    "x_total_valid_std_flat_ddr = encoder.predict(x_total_valid_std_flat)\n",
    "x_total_test_std_flat_ddr = encoder.predict(x_total_test_std_flat)\n",
    "x_couple_train_std_flat_ddr = encoder.predict(x_couple_train_std_flat)\n",
    "x_couple_valid_std_flat_ddr = encoder.predict(x_couple_valid_std_flat)\n",
    "x_couple_test_std_flat_ddr = encoder.predict(x_couple_test_std_flat)\n",
    "\n",
    "\n",
    "if DISABLE_DDR:\n",
    "\tx_total_train_std_flat_ddr = x_total_train_std_flat\n",
    "\tx_total_valid_std_flat_ddr = x_total_valid_std_flat\n",
    "\tx_total_test_std_flat_ddr = x_total_test_std_flat\n",
    "\tx_couple_train_std_flat_ddr = x_couple_train_std_flat\n",
    "\tx_couple_valid_std_flat_ddr = x_couple_valid_std_flat\n",
    "\tx_couple_test_std_flat_ddr = x_couple_test_std_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d9ff1",
   "metadata": {},
   "source": [
    "## 命名简化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aaac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total \n",
    "xt_train = x_total_train_std_flat_ddr\n",
    "xt_valid = x_total_valid_std_flat_ddr\n",
    "xt_test = x_total_test_std_flat_ddr\n",
    "yt_train = y_total_train[:, 1].reshape(-1, 1)\n",
    "yt_valid = y_total_valid[:, 1].reshape(-1, 1)\n",
    "yt_test = y_total_test[:, 1].reshape(-1, 1)\n",
    "\n",
    "# couple\n",
    "xc_train = x_couple_train_std_flat_ddr\n",
    "xc_valid = x_couple_valid_std_flat_ddr\n",
    "xc_test = x_couple_test_std_flat_ddr\n",
    "yc_train = y_couple_train[:, 1].reshape(-1, 1)\n",
    "yc_valid = y_couple_valid[:, 1].reshape(-1, 1)\n",
    "yc_test = y_couple_test[:, 1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469b93d",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d08157",
   "metadata": {},
   "source": [
    "#### 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3e8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.analysis import model_analysis\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35226a33",
   "metadata": {},
   "source": [
    "### 线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc7d03-b048-4937-80ba-51c62861b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "# total\n",
    "print('--------------------------------total------------------------------')\n",
    "lr_t = LinearRegression()\n",
    "lr_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(lr_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'linear reg')\n",
    "results_total = pd.Series(dict_total).to_frame().T\n",
    "\n",
    "# couple\n",
    "print('--------------------------------couple------------------------------')\n",
    "lr_c = LinearRegression()\n",
    "lr_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(lr_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'linear reg')\n",
    "results_couple = pd.Series(dict_couple).to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85016a7",
   "metadata": {},
   "source": [
    "### 支持向量机回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f4997-0d6c-4b57-b8e0-b58e8b89e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# total\n",
    "print('--------------------------------total------------------------------')\n",
    "# linear svr\n",
    "lr_svf_t = SVR(kernel='linear', max_iter=100000)\n",
    "lr_svf_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(lr_svf_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'linear svr')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "# poly svr\n",
    "poly_svf_t = SVR(kernel='poly')\n",
    "poly_svf_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(poly_svf_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'poly svr')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "# rbf svr\n",
    "rbf_svf_t = SVR(kernel='rbf')\n",
    "rbf_svf_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(rbf_svf_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'rbf svr')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "\n",
    "# couple\n",
    "print('--------------------------------couple------------------------------')\n",
    "# linear svr\n",
    "lr_svf_c = SVR(kernel='linear', max_iter=100000)\n",
    "lr_svf_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(lr_svf_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'linear svr')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)\n",
    "# poly svr\n",
    "poly_svf_c = SVR(kernel='poly')\n",
    "poly_svf_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(poly_svf_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'poly svr')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)\n",
    "# rbf svr\n",
    "rbf_svf_c = SVR(kernel='rbf')\n",
    "rbf_svf_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(rbf_svf_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'rbf svr')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7558ac84",
   "metadata": {},
   "source": [
    "### K近邻回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271249a4-3b5a-49e4-8d27-0ff459682c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# total \n",
    "print('--------------------------------total------------------------------')\n",
    "# uniform knn\n",
    "uni_knn_t = KNeighborsRegressor(n_neighbors=5, weights='uniform')\n",
    "uni_knn_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(uni_knn_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'uniform knn')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "# distance knn\n",
    "dis_knn_t = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "dis_knn_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(dis_knn_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'distance knn')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "\n",
    "# couple\n",
    "print('--------------------------------couple------------------------------')\n",
    "# uniform knn\n",
    "uni_knn_c = KNeighborsRegressor(n_neighbors=5, weights='uniform')\n",
    "uni_knn_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(uni_knn_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'uniform knn')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)\n",
    "# distance knn\n",
    "dis_knn_c = KNeighborsRegressor(n_neighbors=5, weights='distance')\n",
    "dis_knn_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(dis_knn_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'distance knn')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9449a06",
   "metadata": {},
   "source": [
    "### 回归树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# total\n",
    "print('--------------------------------total------------------------------')\n",
    "dtr_t = DecisionTreeRegressor()\n",
    "dtr_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(dtr_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'decision tree total')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "\n",
    "# couple\n",
    "print('--------------------------------couple------------------------------')\n",
    "dtr_c = DecisionTreeRegressor()\n",
    "dtr_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(dtr_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'decision tree couple')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93804e6",
   "metadata": {},
   "source": [
    "### 集成模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9351d3",
   "metadata": {},
   "source": [
    "#### 基础模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "\n",
    "# total\n",
    "print('--------------------------------total------------------------------')\n",
    "# random forest\n",
    "rfr_t = RandomForestRegressor()\n",
    "rfr_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(rfr_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'random forest')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "# extra tree\n",
    "etr_t = ExtraTreesRegressor()\n",
    "etr_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(etr_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'extra tree')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "# gradient boosting\n",
    "gbr_t = GradientBoostingRegressor()\n",
    "gbr_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(gbr_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'gradient boosting')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "\n",
    "# couple\n",
    "print('--------------------------------couple------------------------------')\n",
    "# random forest\n",
    "rfr_c = RandomForestRegressor()\n",
    "rfr_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(rfr_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'random forest')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)\n",
    "# extra tree\n",
    "etr_c = ExtraTreesRegressor()\n",
    "etr_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(etr_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'extra tree')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)\t\n",
    "# gradient boosting\n",
    "gbr_c = GradientBoostingRegressor()\n",
    "gbr_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(gbr_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'gradient boosting')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78662cb7",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# total\n",
    "# XGBoost extreme gradient boosting\n",
    "xgb_t = xgb.XGBRegressor(booster='gbtree',\n",
    "                         n_estimators=100,\n",
    "                         learning_rate=0.1,\n",
    "                         max_depth=6,\n",
    "                         min_child_weight=3,\n",
    "                         seed=42)\n",
    "xgb_t.fit(xt_train, yt_train.ravel())\n",
    "dict_total = model_analysis(xgb_t, xt_train, yt_train, xt_valid, yt_valid, xt_test, yt_test, 'XGBoost')\n",
    "results_total = pd.concat([results_total, pd.Series(dict_total).to_frame().T], axis=0)\n",
    "\n",
    "# couple\n",
    "# XGBoost extreme gradient boosting\n",
    "xgb_c = xgb.XGBRegressor(booster='gbtree',\n",
    "                         n_estimators=100,\n",
    "                         learning_rate=0.1,\n",
    "                         max_depth=6,\n",
    "                         min_child_weight=3,\n",
    "                         seed=42)\n",
    "xgb_c.fit(xc_train, yc_train.ravel())\n",
    "dict_couple = model_analysis(xgb_c, xc_train, yc_train, xc_valid, yc_valid, xc_test, yc_test, 'XGBoost')\n",
    "results_couple = pd.concat([results_couple, pd.Series(dict_couple).to_frame().T], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286078b",
   "metadata": {},
   "source": [
    "### 懒模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# total\n",
    "print('--------------------------------total------------------------------')\n",
    "reg_t = LazyRegressor(verbose=0, custom_metric=ratio_good, predictions=True, ignore_warnings=True)\n",
    "models_t, predictions_t = reg_t.fit(xt_train, xt_valid, yt_train.ravel(), yt_valid.ravel())\n",
    "print(models_t)\n",
    "\n",
    "# couple\n",
    "print('--------------------------------couple------------------------------')\n",
    "reg_c = LazyRegressor(verbose=0, custom_metric=ratio_good, predictions=True, ignore_warnings=True)\n",
    "models_c, predictions_c = reg_c.fit(xc_train, xc_valid, yc_train.ravel(), yc_valid.ravel())\n",
    "print(models_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1cc5b4",
   "metadata": {},
   "source": [
    "## 结果存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results_total and results_couple to csv\n",
    "dir_results = os.path.join(dir_prj, \"results\")\n",
    "if not os.path.exists(dir_results):\n",
    "\tos.mkdir(dir_results)\n",
    "\n",
    "results_total.to_csv(os.path.join(dir_results, \"base_total.csv\"), index=False)\n",
    "results_couple.to_csv(os.path.join(dir_results, \"base_couple.csv\"), index=False)\n",
    "\n",
    "# save lazyregressor results to csv\n",
    "models_t.to_csv(os.path.join(dir_results, \"lazy_models_total.csv\"), index=True)\n",
    "models_c.to_csv(os.path.join(dir_results, \"lazy_models_couple.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
